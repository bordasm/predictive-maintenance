{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: Model Building & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete successfully!\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import urllib\n",
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# https://www.intel.com/content/www/us/en/developer/articles/guide/guide-to-tensorflow-runtime-optimizations-for-cpu.html\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(70) \n",
    "#tf.config.threading.set_intra_op_parallelism_threads(70)\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "\n",
    "print('Import complete successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and dump a short summary of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/intermediateData/PdM_telemetry_model_age_16_maint_err_fail_RUL_cycle_train.csv', sep=\",\", header=0)\n",
    "train_df.loc[train_df['failures'] == '-', 'failures'] = 0\n",
    "for fail in train_df['failures']:\n",
    "    if fail != 0:\n",
    "        train_df.loc[train_df['failures'] == fail, 'failures'] = int(fail)\n",
    "train_df_sorted = train_df.sort_values(by=['machineID', 'datetime'], ascending=[True, False])\n",
    "for index, row in train_df_sorted.iterrows():\n",
    "    if row['failures'] != 0 and row['label1'] == 1:\n",
    "        failures_prev = row['failures']\n",
    "    elif row['failures'] == 0 and row['label1'] == 1:\n",
    "        train_df_sorted.loc[train_df_sorted.index == index, 'failures'] = failures_prev\n",
    "train_df = train_df_sorted.sort_values(by=['machineID', 'datetime'], ascending=[True, True])\n",
    "train_df.to_csv('./data/intermediateData/PM_train_pre_prepared.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/intermediateData/PM_test_almost_prepared.csv', sep=\",\", header=0)\n",
    "test_df.loc[test_df['failures'] == '-', 'failures'] = 0\n",
    "for fail in test_df['failures']:\n",
    "    if fail != 0:\n",
    "        test_df.loc[train_df['failures'] == fail, 'failures'] = int(fail)\n",
    "test_df_sorted = test_df.sort_values(by=['machineID', 'datetime'], ascending=[True, False])\n",
    "for index, row in test_df_sorted.iterrows():\n",
    "    if row['failures'] != 0 and row['label1'] == 1:\n",
    "        failures_prev = row['failures']\n",
    "    elif row['failures'] == 0 and row['label1'] == 1:\n",
    "        test_df_sorted.loc[test_df_sorted.index == index, 'failures'] = failures_prev\n",
    "test_df = test_df_sorted.sort_values(by=['machineID', 'datetime'], ascending=[True, True])\n",
    "test_df.to_csv('./data/intermediateData/PM_test_pre_prepared.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/intermediateData/PM_train_pre_prepared.csv', sep=\",\", header=0)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balancing training data\n",
    "#print('Balancing train data is in progress...')\n",
    "df_train = train_df[[\"datetime\",\"machineID\",\"cycle\",\"model\",\"age\",\"volt\",\"rotate\",\"pressure\",\"vibration\",\"RUL\",\"label1\",\"label2\",\"cycle_norm\"]]\n",
    "y_train = train_df[\"failures\"]\n",
    "#print(df_train.shape)\n",
    "#print(y_train.shape)\n",
    "#rus = RandomUnderSampler(sampling_strategy={0:30000})\n",
    "rus = RandomUnderSampler(sampling_strategy={0:19935})\n",
    "#rus = RandomUnderSampler()\n",
    "df_resampled, y_resampled = rus.fit_resample(df_train, y_train)\n",
    "#print(df_resampled.shape)\n",
    "#print(y_resampled.shape)\n",
    "df_balanced = pd.concat([df_resampled, y_resampled],axis=1)\n",
    "train_df = df_balanced.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "train_df = train_df[['datetime','machineID', 'cycle', 'model', 'age', 'volt', 'rotate', 'pressure', 'vibration', 'failures', 'RUL', 'cycle_norm', 'label1', 'label2']]\n",
    "#check balanced data\n",
    "#train_df.to_csv('./data/PM_train_prepared_balanced_datetime.csv',index=False)\n",
    "#print(train_df.shape)\n",
    "#train_df.drop(train_df.columns[[0]], axis=1, inplace=True)\n",
    "#check balanced data\n",
    "#train_df.to_csv('./data/intermediateData/PM_train_prepared_balanced.csv',index=False)\n",
    "train_df\n",
    "#print('Balancing train data completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the following categories with their predecessor data rows (failure 0-s)\n",
    "dfb1 = train_df\n",
    "dfb1 = dfb1.sort_values(['machineID', 'datetime'], ascending=[False, False])\n",
    "dfb1.to_csv('./data/intermediateData/PM_train_prepared_1_sorted.csv',index=False)\n",
    "with open(\"data/intermediateData/PM_train_prepared_1_sorted.csv\", \"r\", newline='') as sourceb1:\n",
    "    readerb1 = csv.reader(sourceb1)\n",
    "      \n",
    "    with open(\"data/intermediateData/PM_train_prepared_balanced_2_filtered_unsorted.csv\", \"w\", newline='') as resultb1:\n",
    "        writerb1 = csv.writer(resultb1)\n",
    "        failprev=False\n",
    "        failprevzero=False\n",
    "        for rb1 in readerb1:\n",
    "            if rb1[0] == 'datetime':\n",
    "                writerb1.writerow(rb1)\n",
    "            elif rb1[9] in (str(1),str(2),str(4)) and failprev == False:\n",
    "                writerb1.writerow(rb1)\n",
    "                failprev = True\n",
    "            elif rb1[9] in (str(1),str(2),str(4)) and failprev == True:\n",
    "                writerb1.writerow(rb1)\n",
    "            elif rb1[9] == str(0) and failprev == True and failprevzero == False:\n",
    "                writerb1.writerow(rb1)\n",
    "                failprevzero = True\n",
    "            elif rb1[9] == str(0) and failprev == True and failprevzero == True:\n",
    "                writerb1.writerow(rb1)\n",
    "            \n",
    "sourceb1.close()\n",
    "resultb1.close()\n",
    "\n",
    "#print('Writing of .../data/intermediateData/PM_train_prepared_balanced_2_filtered_unsorted.csv completed successfully.')\n",
    "\n",
    "dfb2 = pd.read_csv('./data/intermediateData/PM_train_prepared_balanced_2_filtered_unsorted.csv', sep=\",\", header=0)\n",
    "dfb2 = dfb2.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "dfb2.drop(train_df.columns[[0]], axis=1, inplace=True)\n",
    "dfb2.to_csv('./data/preparedData/PM_train_prepared.csv',index=False)\n",
    "train_df = dfb2\n",
    "train_df\n",
    "\n",
    "#print('Writing of .../data/intermediateData/PM_train_prepared_balanced_3_filtered_sorted.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the following categories with their predecessor data rows (failure 0-s)\n",
    "dfb3 = test_df\n",
    "dfb3 = dfb3.sort_values(['machineID', 'datetime'], ascending=[False, False])\n",
    "dfb3.to_csv('./data/intermediateData/PM_test_almost_prepared_1_sorted.csv',index=False)\n",
    "with open(\"data/intermediateData/PM_test_almost_prepared_1_sorted.csv\", \"r\", newline='') as sourceb3:\n",
    "    readerb3 = csv.reader(sourceb3)\n",
    "      \n",
    "    with open(\"data/intermediateData/PM_test_almost_prepared_balanced_2_filtered_unsorted.csv\", \"w\", newline='') as resultb3:\n",
    "        writerb3 = csv.writer(resultb3)\n",
    "        failprev=False\n",
    "        failprevzero=False\n",
    "        for rb3 in readerb3:\n",
    "            if rb3[0] == 'datetime':\n",
    "                writerb3.writerow(rb3)\n",
    "            elif rb3[9] in (str(1),str(2),str(4)) and failprev == False:\n",
    "                writerb3.writerow(rb3)\n",
    "                failprev = True\n",
    "            elif rb3[9] in (str(1),str(2),str(4)) and failprev == True:\n",
    "                writerb3.writerow(rb3)\n",
    "            elif rb3[9] == str(0) and failprev == True and failprevzero == False:\n",
    "                writerb3.writerow(rb3)\n",
    "                failprevzero = True\n",
    "            elif rb3[9] == str(0) and failprev == True and failprevzero == True:\n",
    "                writerb3.writerow(rb3)\n",
    "            \n",
    "sourceb3.close()\n",
    "resultb3.close()\n",
    "\n",
    "#print('Writing of .../data/intermediateData/PM_train_prepared_balanced_2_filtered_unsorted.csv completed successfully.')\n",
    "\n",
    "dfb4 = pd.read_csv('./data/intermediateData/PM_test_almost_prepared_balanced_2_filtered_unsorted.csv', sep=\",\", header=0)\n",
    "dfb4 = dfb4.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "dfb4.drop(test_df.columns[[0]], axis=1, inplace=True)\n",
    "#test_df.drop(test_df.columns[[0]], axis=1, inplace=True)\n",
    "dfb4.to_csv('./data/preparedData/PM_test_prepared.csv',index=False)\n",
    "test_df = dfb4\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   machineID  cycle     model  age      volt    rotate  pressure  vibration  \\\n",
      "0          1     17  0.666667  0.9  0.342238  0.550659  0.319269   0.440007   \n",
      "1          1     19  0.666667  0.9  0.524516  0.625741  0.337727   0.387953   \n",
      "2          1     24  0.666667  0.9  0.531867  0.642028  0.313270   0.605337   \n",
      "3          1     67  0.666667  0.9  0.620057  0.534940  0.390426   0.566431   \n",
      "4          1     68  0.666667  0.9  0.453993  0.590838  0.489264   0.702829   \n",
      "5          1     69  0.666667  0.9  0.430023  0.604926  0.470232   0.689213   \n",
      "6          1     70  0.666667  0.9  0.600360  0.661463  0.463462   0.693415   \n",
      "7          1     71  0.666667  0.9  0.622540  0.441451  0.410613   0.561752   \n",
      "8          1     72  0.666667  0.9  0.464634  0.585275  0.389755   0.703889   \n",
      "9          1     73  0.666667  0.9  0.428900  0.557030  0.344969   0.538842   \n",
      "\n",
      "   failures  RUL  cycle_norm  label1  label2  \n",
      "0         0   80    0.002470       0       0  \n",
      "1         0   78    0.002778       0       0  \n",
      "2         0   73    0.003550       0       0  \n",
      "3         4   30    0.010187       1       1  \n",
      "4         4   29    0.010341       1       1  \n",
      "5         4   28    0.010495       1       1  \n",
      "6         4   27    0.010650       1       1  \n",
      "7         4   26    0.010804       1       1  \n",
      "8         4   25    0.010958       1       1  \n",
      "9         4   24    0.011113       1       1  \n",
      "   machineID  cycle     model       age      volt    rotate  pressure  \\\n",
      "0         89      1  0.666667  0.833333  0.420252  0.816190  0.653248   \n",
      "1         89      2  0.666667  0.833333  0.408611  0.762715  0.829219   \n",
      "2         89      3  0.666667  0.833333  0.250609  0.301397  0.657232   \n",
      "3         89      4  0.666667  0.833333  0.497216  0.554726  0.809499   \n",
      "4         89      5  0.666667  0.833333  0.418949  0.471659  0.482283   \n",
      "5         89      6  0.666667  0.833333  0.523158  0.518877  0.767339   \n",
      "6         89      7  0.666667  0.833333  0.457698  0.741963  0.429188   \n",
      "7         89      8  0.666667  0.833333  0.437202  0.455160  0.392089   \n",
      "8         89      9  0.666667  0.833333  0.477320  0.468083  0.469835   \n",
      "9         89     10  0.666667  0.833333  0.492070  0.599858  0.345500   \n",
      "\n",
      "   vibration  failures  RUL  cycle_norm  label1  label2  \n",
      "0   0.549285         0  888    0.000000       0       0  \n",
      "1   0.476602         0  887    0.000198       0       0  \n",
      "2   0.357829         0  886    0.000397       0       0  \n",
      "3   0.160269         0  885    0.000595       0       0  \n",
      "4   0.490419         0  884    0.000794       0       0  \n",
      "5   0.514914         0  883    0.000992       0       0  \n",
      "6   0.376204         0  882    0.001191       0       0  \n",
      "7   0.416383         0  881    0.001389       0       0  \n",
      "8   0.461749         0  880    0.001588       0       0  \n",
      "9   0.507347         0  879    0.001786       0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/preparedData/PM_train_prepared.csv', sep=\",\", header=0)\n",
    "print(train_df.head(10))\n",
    "test_df = pd.read_csv('./data/preparedData/PM_test_prepared.csv', sep=\",\", header=0)\n",
    "test_df.loc[test_df['failures'] == '-', 'failures'] = 0\n",
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model', 'age', 'volt', 'rotate', 'pressure', 'vibration']\n",
      "['cycle_norm', 'model', 'age', 'volt', 'rotate', 'pressure', 'vibration']\n"
     ]
    }
   ],
   "source": [
    "# pick the feature columns \n",
    "sequence_cols = ['cycle_norm']\n",
    "key_cols = ['machineID', 'cycle']\n",
    "label_cols = ['failures', 'label1', 'label2','RUL']\n",
    "\n",
    "input_features = test_df.columns.values.tolist()\n",
    "sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "# The time is sequenced along\n",
    "# This may be a silly way to get these column names, but it's relatively clear\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "print(sensor_cols)\n",
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24717, 120, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator for the sequences\n",
    "#sequence_cols = train_df.columns.tolist()\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['machineID']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['machineID'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "#print(list(seq_gen))\n",
    "#with open('seq_gen_list.txt', 'w') as f:\n",
    "#    for item in seq_gen:\n",
    "#        f.write(\"%s\\n\" % item)\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(24717, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(24717, 4)\n"
     ]
    }
   ],
   "source": [
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['machineID']==id], sequence_length, ['failures']) \n",
    "             for id in train_df['machineID'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array_orig = label_array\n",
    "print(type(label_array))\n",
    "print(label_array.shape)\n",
    "#print(label_array)\n",
    "\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "df_label_array = pd.DataFrame(label_array)\n",
    "#reshape the 1-D label_array and finally fit the object \n",
    "X = onehotencoder.fit_transform(df_label_array.values.reshape(-1,1)).toarray()\n",
    "df_label_array = pd.DataFrame(X) \n",
    "#printing to verify\n",
    "#df_label_array.to_csv('./data/df_label_array.csv',index=False)\n",
    "label_array = df_label_array.to_numpy()\n",
    "print(type(label_array))\n",
    "print(label_array.shape)\n",
    "#print(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If theres is an existing model.\n",
    "model = keras.models.load_model('models/machineErrorTypePredictor_for_type_1_2_4.h5')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 7)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 120, 7)      253959      ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 120, 7)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 120, 7)      14          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 120, 7)      0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 120, 8)       64          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 8)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 120, 7)       63          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 120, 7)      14          ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 120, 7)      253959      ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 120, 7)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 120, 7)      14          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 120, 8)       64          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 120, 8)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 120, 7)       63          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 120, 7)      14          ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 120, 7)      253959      ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 120, 7)       0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 120, 7)      14          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 120, 8)       64          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 120, 8)       0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 120, 7)       63          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 120, 7)      14          ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 120, 7)      253959      ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 120, 7)       0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 120, 7)      14          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 120, 8)       64          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 120, 8)       0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 120, 7)       63          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 120, 7)      14          ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 120, 7)      253959      ['tf.__operators__.add_7[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 120, 7)       0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 120, 7)      14          ['dropout_8[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_8[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 120, 8)       64          ['tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 120, 8)       0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 120, 7)       63          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 120, 7)      14          ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 120, 7)      0           ['layer_normalization_9[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 120, 7)      253959      ['tf.__operators__.add_9[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 120, 7)       0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 120, 7)      14          ['dropout_10[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 120, 7)      0           ['layer_normalization_10[0][0]', \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 120, 8)       64          ['tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 120, 8)       0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 120, 7)       63          ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 120, 7)      14          ['conv1d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 120, 7)      0           ['layer_normalization_11[0][0]', \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 120)         0           ['tf.__operators__.add_11[0][0]']\n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          30976       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            1028        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,556,688\n",
      "Trainable params: 1,556,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "input_shape = seq_array.shape[1:]\n",
    "n_classes = label_array.shape[1]\n",
    "#nb_features = seq_array.shape[2]\n",
    "#nb_out = label_array.shape[1]\n",
    "head_size=1024\n",
    "num_heads=8\n",
    "ff_dim=8\n",
    "num_transformer_blocks=6\n",
    "mlp_units=[256]\n",
    "mlp_dropout=0.4\n",
    "dropout=0.25\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = inputs\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "for dim in mlp_units:\n",
    "    x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(mlp_dropout)(x)\n",
    "outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "#lr_schedule = 1e-4\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"CategoricalAccuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24717, 120, 7)\n",
      "(24717, 4)\n"
     ]
    }
   ],
   "source": [
    "print(seq_array.shape)\n",
    "print(label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "541/541 [==============================] - 22981s 42s/step - loss: 1.0829 - categorical_accuracy: 0.5522 - val_loss: 0.9178 - val_categorical_accuracy: 0.6133\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 22824s 42s/step - loss: 0.9136 - categorical_accuracy: 0.6316 - val_loss: 0.8850 - val_categorical_accuracy: 0.6413\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 22494s 42s/step - loss: 0.9327 - categorical_accuracy: 0.6252 - val_loss: 0.8837 - val_categorical_accuracy: 0.6482\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 22464s 42s/step - loss: 0.9025 - categorical_accuracy: 0.6326 - val_loss: 0.9682 - val_categorical_accuracy: 0.6134\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 22342s 41s/step - loss: 0.9889 - categorical_accuracy: 0.5909 - val_loss: 0.9448 - val_categorical_accuracy: 0.6166\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 22258s 41s/step - loss: 0.9357 - categorical_accuracy: 0.6155 - val_loss: 0.9686 - val_categorical_accuracy: 0.5876\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 22030s 41s/step - loss: 0.9343 - categorical_accuracy: 0.6165 - val_loss: 0.9070 - val_categorical_accuracy: 0.6156\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 22058s 41s/step - loss: 0.9902 - categorical_accuracy: 0.5818 - val_loss: 0.9131 - val_categorical_accuracy: 0.6185\n",
      "CPU times: total: 30d 11h 45min 56s\n",
      "Wall time: 2d 1h 50min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21085122400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit the network\n",
    "earystopping = [keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', # Monitor the validation categorical accuracy\n",
    "                                                     min_delta=0,    # until it doesn't change (or gets worse)\n",
    "                                                     patience=5,  # patience > 1 so it continutes if it is not consistently improving\n",
    "                                                     verbose=0, \n",
    "                                                     mode='auto',\n",
    "                                                     restore_best_weights=True)]\n",
    "\n",
    "modelcheckpoint = (keras.callbacks.ModelCheckpoint('machineErrorTypePredictor_for_type_1_2_4.h5',\n",
    "                                    monitor='val_categorical_accuracy',\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=False))\n",
    "\n",
    "model.fit(seq_array, # Training features\n",
    "          label_array, # Training labels\n",
    "          validation_split=0.3,\n",
    "          epochs=20,\n",
    "          batch_size=32,\n",
    "          verbose=1,\n",
    "          callbacks=[earystopping,modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('models/machineErrorTypePredictor_for_type_1_2_4.h5')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 11270s 91s/step - loss: 0.8423 - categorical_accuracy: 0.6699\n",
      "Training Accurracy: 0.6699437499046326\n"
     ]
    }
   ],
   "source": [
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Training Accurracy: {}'.format(scores[1]))\n",
    "# run_logger.log(\"Training Accuracy\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 11240s 91s/step\n",
      "Training Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13181,     0,   255,    73,     0],\n",
       "       [ 2878,     0,     9,   220,     0],\n",
       "       [ 2912,     0,  2040,     0,     0],\n",
       "       [    0,     0,     0,     0,     0],\n",
       "       [ 2231,     0,     0,   918,     0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "y_pred = (model.predict(seq_array,verbose=1, batch_size=200) > 0.5).astype(\"int32\")\n",
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "y_true = label_array_orig\n",
    "print('Training Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Precision:  0.5171723131074082 \n",
      " Training Recall:  0.6158109802969616 \n",
      " Training F1 Score: 0.5621978558221374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print( 'Training Precision: ', precision, '\\n', 'Training Recall: ', recall, '\\n', 'Training F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 120, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_array_test_last = [test_df[test_df['machineID']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['machineID'].unique() if len(test_df[test_df['machineID']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "seq_array_test_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 120, 7)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "y_mask = [len(test_df[test_df['machineID']==id]) >= sequence_length for id in test_df['machineID'].unique()]\n",
    "\n",
    "label_array_test_last = test_df.groupby('machineID')['failures'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "label_array_test_last.shape\n",
    "\n",
    "print(seq_array_test_last.shape)\n",
    "print(label_array_test_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 4s - loss: 1.1927 - categorical_accuracy: 0.2500 - 4s/epoch - 4s/step\n",
      "Test Accurracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "# test metrics\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "df_label_array_test_last = pd.DataFrame(label_array_test_last)\n",
    "#reshape the 1-D label_array and finally fit the object \n",
    "X = onehotencoder.fit_transform(df_label_array_test_last.values.reshape(-1,1)).toarray()\n",
    "df_label_array_test_last = pd.DataFrame(X) \n",
    "#printing to verify\n",
    "#df_label_array_test_last.to_csv('./data/df_label_array_test_last.csv',index=False)\n",
    "#print(df_label_array_test_last.head())\n",
    "label_array_test_last = df_label_array_test_last.to_numpy()\n",
    "#print(type(label_array_test_last))\n",
    "#print(label_array_test_last.shape)\n",
    "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "print('Test Accurracy: {}'.format(scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [2, 0, 0, 0],\n",
       "       [3, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions and compute confusion matrix\n",
    "y_pred_test = (model.predict(seq_array_test_last,verbose=1, batch_size=200) > 0.5).astype(\"int32\")\n",
    "y_true_test = label_array_test_last\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true_test.argmax(axis=1), y_pred_test.argmax(axis=1))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision:  0.05 \n",
      " Test Recall:  0.125 \n",
      " Test F1 Score: 0.07142857142857144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "precision_test = precision_score(y_true_test, y_pred_test, average='weighted')\n",
    "recall_test = recall_score(y_true_test, y_pred_test, average='weighted')\n",
    "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "print( 'Test Precision: ', precision_test, '\\n', 'Test Recall: ', recall_test, '\\n', 'Test F1 Score:', f1_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ad550f9010c2abc8cfb8fd377ed6984623ccd8f789bba9ce86b891091d41c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
