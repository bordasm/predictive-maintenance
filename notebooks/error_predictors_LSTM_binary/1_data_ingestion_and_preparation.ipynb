{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Ingestion & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the libraries is in progress...\n",
      "Import complete successfully!\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "print(\"Importing the libraries is in progress...\")\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import socket\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import urllib\n",
    "\n",
    "print(\"Import complete successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation step 1 in progress...\n",
      "Writing of PdM_01_telemetry_model_age.csv completed successfully.\n",
      "Data preparation step 1 complete.\n"
     ]
    }
   ],
   "source": [
    "# data preparation step 1\n",
    "# put into PdM_telemetry.csv for 3. and 4. columns the model and age columns from PdM_machines.csv\n",
    "\n",
    "print('Data preparation step 1 in progress...')\n",
    "\n",
    "inp1 = open('./data/00_sourceData/PdM_telemetry.csv', 'r', newline='')\n",
    "reader1 = csv.reader(inp1, delimiter=',')\n",
    "\n",
    "inp2 = open('./data/00_sourceData/PdM_machines.csv', 'r', newline='')\n",
    "reader2 = list(csv.reader(inp2, delimiter=','))\n",
    "\n",
    "with open('./data/01_intermediateData/PdM_01_telemetry_model_age.csv', 'w', newline='') as out1:\n",
    "    writer1 = csv.writer(out1)\n",
    "    for row1 in reader1:\n",
    "        if row1[0] == \"datetime\":\n",
    "            writer1.writerow([row1[0]]+[row1[1]]+[\"model\"]+[\"age\"]+[row1[2]]+[row1[3]]+[row1[4]]+[row1[5]])\n",
    "        else:\n",
    "            for row2 in reader2:\n",
    "                if row2[0] == \"machineID\":\n",
    "                    continue\n",
    "                elif row2[0] == row1[1]:\n",
    "                    writer1.writerow([row1[0]]+[row1[1]]+[row2[1]]+[row2[2]]+[row1[2]]+[row1[3]]+[row1[4]]+[row1[5]])\n",
    "                    \n",
    "inp1.close()\n",
    "inp2.close()\n",
    "out1.close()\n",
    "\n",
    "print('Writing of PdM_01_telemetry_model_age.csv completed successfully.')\n",
    "print('Data preparation step 1 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation step 2 in progress...\n",
      "Writing of PdM_02_telemetry_model_age_maint_pre.csv completed successfully.\n",
      "Data preparation step 2 complete.\n"
     ]
    }
   ],
   "source": [
    "# data preparation step 2\n",
    "# put into PdM_01_telemetry_model_age.csv for last column the comp column from PdM_maint.csv\n",
    "\n",
    "print('Data preparation step 2 in progress...')\n",
    "\n",
    "inp3 = open('./data/01_intermediateData/PdM_01_telemetry_model_age.csv', 'r', newline='')\n",
    "reader3 = csv.reader(inp3, delimiter=',')\n",
    "\n",
    "inp4 = open('./data/00_sourceData/PdM_maint.csv', 'r', newline='')\n",
    "reader4 = list(csv.reader(inp4, delimiter=','))\n",
    "\n",
    "with open('./data/01_intermediateData/PdM_02_telemetry_model_age_maint_pre.csv', 'w', newline='') as out2:\n",
    "    writer2 = csv.writer(out2)\n",
    "    for row3 in reader3:\n",
    "        if row3[0] == \"datetime\":\n",
    "            writer2.writerow(row3+[\"maintained_component\"])\n",
    "        else:\n",
    "            compExist = 0\n",
    "            for row4 in reader4:\n",
    "                if row4[0] == row3[0]:\n",
    "                    if row4[1] == row3[1]:\n",
    "                        writer2.writerow(row3+[row4[2]])\n",
    "                        compExist = 1\n",
    "            else:\n",
    "                if compExist == 0:\n",
    "                    writer2.writerow(row3+[\"-\"])\n",
    "                    \n",
    "inp3.close()\n",
    "inp4.close()\n",
    "out2.close()\n",
    "\n",
    "print('Writing of PdM_02_telemetry_model_age_maint_pre.csv completed successfully.')\n",
    "print('Data preparation step 2 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation step 3 in progress...\n",
      "Writing of PdM_03_telemetry_model_age_maint_final.csv completed successfully.\n",
      "Data preparation step 3 complete.\n"
     ]
    }
   ],
   "source": [
    "# data preparation step 3\n",
    "# combine (concatenate) data from multiple rows in CSV into one cell in another CSV in PdM_telemetry_model_age_02_maint_pre.csv for last column\n",
    "\n",
    "print('Data preparation step 3 in progress...')\n",
    "\n",
    "d_telem = collections.defaultdict(set)\n",
    "\n",
    "with open(\"./data/01_intermediateData/PdM_02_telemetry_model_age_maint_pre.csv\", \"r\") as f_input3:\n",
    "    csv_input3 = csv.reader(f_input3, skipinitialspace=True)\n",
    "    headers = next(csv_input3)\n",
    "\n",
    "    for row in csv_input3:\n",
    "        d_telem[row[0]+\",\"+row[1]+\",\"+row[2]+\",\"+row[3]+\",\"+row[4]+\",\"+row[5]+\",\"+row[6]+\",\"+row[7]].add(row[8])\n",
    "        #d_telem[row[0]].append(row[1])   # if a list is preferred\n",
    "\n",
    "with open(\"./data/01_intermediateData/PdM_03_telemetry_model_age_maint_final.csv\", \"w\") as f_output2:\n",
    "    csv_output2 = csv.writer(f_output2)\n",
    "    csv_output2.writerow(headers)\n",
    "\n",
    "    telem_data = d_telem.keys()\n",
    "\n",
    "    for telem in telem_data:\n",
    "        l_maint = list(d_telem[telem])\n",
    "        csv_output2.writerow([telem, \"+\".join(l_maint)])\n",
    "\n",
    "f_input3.close()\n",
    "f_output2.close()\n",
    "\n",
    "# Jelen allas szerint a PdM_03_telemetry_model_age_maint_final.csv-bol manualisan el kell tavolitani az osszes \"-et es az osszes ures sort (gyakorlatban: \\r),\n",
    "# es akkor lesz jo a fajl.\n",
    "\n",
    "print('Writing of PdM_03_telemetry_model_age_maint_final.csv completed successfully.')\n",
    "print('Data preparation step 3 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation step 4 skipped because we do not need error data.\n"
     ]
    }
   ],
   "source": [
    "# data preparation step 4 skipped because we don't need error data only failure data\n",
    "# put into PdM_03_telemetry_model_age_maint_final.csv for last column the errorID column from PdM_errors.csv\n",
    "\n",
    "print ('Data preparation step 4 skipped because we do not need error data only failure data.')\n",
    "# print('Data preparation step 4 in progress...')\n",
    "\n",
    "#inp5 = open('./data/01_intermediateData/PdM_03_telemetry_model_age_maint_final.csv', 'r', newline='')\n",
    "#reader5 = csv.reader(inp5, delimiter=',')\n",
    "\n",
    "#inp6 = open('./data/00_sourceData/PdM_errors.csv', 'r', newline='')\n",
    "#reader6 = list(csv.reader(inp6, delimiter=','))\n",
    "\n",
    "#with open('./data/01_intermediateData/PdM_04_telemetry_model_age_maint_err_pre.csv', 'w', newline='') as out3:\n",
    "#    writer3 = csv.writer(out3)\n",
    "#    for row5 in reader5:\n",
    "#        if row5[0] == \"datetime\":\n",
    "#            writer3.writerow(row5+[\"errors_failures\"])\n",
    "#        else:\n",
    "#            errExist = 0\n",
    "#            for row6 in reader6:\n",
    "#                if row6[0] == row5[0]:\n",
    "#                    if row6[1] == row5[1]:\n",
    "#                        writer3.writerow(row5+[row6[2][-1:]])\n",
    "#                        errExist = 1\n",
    "#            else:\n",
    "#                if errExist == 0:\n",
    "#                    writer3.writerow(row5+[\"0\"])\n",
    "                    \n",
    "#inp5.close()\n",
    "#inp6.close()\n",
    "#out3.close()\n",
    "\n",
    "#print('Writing of PdM_04_telemetry_model_age_maint_err_pre.csv completed successfully.')\n",
    "#print('Data preparation step 4 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation step 5 in progress...\n",
      "Writing of PdM_05_telemetry_model_age_maint_fail.csv completed successfully.\n",
      "Data preparation step 5 complete.\n"
     ]
    }
   ],
   "source": [
    "# data preparation step 5\n",
    "# put into PdM_telemetry_model_age_05_maint_final.csv to the last column (called failures) the coded value of failure column from PdM_failures.csv\n",
    "\n",
    "print('Data preparation step 5 in progress...')\n",
    "\n",
    "inp7 = open('./data/01_intermediateData/PdM_03_telemetry_model_age_maint_final.csv', 'r', newline='')\n",
    "reader7 = csv.reader(inp7, delimiter=',')\n",
    "\n",
    "inp8 = open('./data/00_sourceData/PdM_failures.csv', 'r', newline='')\n",
    "reader8 = list(csv.reader(inp8, delimiter=','))\n",
    "\n",
    "with open('./data/01_intermediateData/PdM_05_telemetry_model_age_maint_fail.csv', 'w', newline='') as out4:\n",
    "    writer4 = csv.writer(out4)\n",
    "    for row7 in reader7:\n",
    "        if row7[0] == \"datetime\":\n",
    "            writer4.writerow(row7+[\"failures\"])\n",
    "        else:\n",
    "            failExist = 0\n",
    "            for row8 in reader8:\n",
    "                if row8[0] == row7[0]:\n",
    "                    if row8[1] == row7[1]:\n",
    "                        writer4.writerow(row7+[row8[2][-1:]+\"0\"])\n",
    "                        failExist = 1\n",
    "            else:\n",
    "                if failExist == 0:\n",
    "                    writer4.writerow(row7+[\"0\"])\n",
    "                    \n",
    "inp7.close()\n",
    "inp8.close()\n",
    "out4.close()\n",
    "\n",
    "print('Writing of PdM_05_telemetry_model_age_maint_fail.csv completed successfully.')\n",
    "print('Data preparation step 5 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting data is in progress...\n",
      "Writing of .../data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sorting input file by machineID (ascending) and datetime (descending) to determine RUL\n",
    "print('Sorting data is in progress...')\n",
    "\n",
    "df1 = pd.read_csv('./data/01_intermediateData/PdM_05_telemetry_model_age_maint_fail.csv', delimiter=',')\n",
    "df1 = df1.sort_values(['machineID', 'datetime'], ascending=[True, False])\n",
    "df1.to_csv('./data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv',index=False)\n",
    "\n",
    "print('Writing of .../data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine RUL is in progress...\n",
      "failure1 RUL determination is in progress...\n",
      "failure2 RUL determination is in progress...\n",
      "failure3 RUL determination is in progress...\n",
      "failure4 RUL determination is in progress...\n",
      "Writing of .../data/01_intermediateData/PdM_07_RULn.csv files completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Determine RUL and break down the data by error type and save them in separate csv files\n",
    "print('Determine RUL is in progress...')\n",
    "\n",
    "# failure1\n",
    "print('failure1 RUL determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv\", \"r\", newline='') as source910:\n",
    "    reader910 = csv.reader(source910)\n",
    "    with open(\"data/01_intermediateData/PdM_07_RUL10.csv\", \"w\", newline='') as result910:\n",
    "        writer510 = csv.writer(result910)\n",
    "        writeData = 0\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 0\n",
    "        for r910 in reader910:\n",
    "            if r910[0] == \"datetime\":\n",
    "                writer510.writerow(r910+[\"RUL\"])\n",
    "            else:\n",
    "                mID = int(r910[1])\n",
    "                if mID != mIDprev:\n",
    "                    writeData = 0\n",
    "                    mIDprev = mID\n",
    "                else:\n",
    "                    if r910[9] == \"10\":\n",
    "                        writeData = 1\n",
    "                        i = 0\n",
    "                        writer510.writerow(r910+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r910[9] == \"0\":\n",
    "                        writer510.writerow(r910+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r910[9] != \"0\" and r910[9] != \"10\":\n",
    "                        writer510.writerow([r910[0]]+[r910[1]]+[r910[2]]+[r910[3]]+[r910[4]]+[r910[5]]+[r910[6]]+[r910[7]]+[r910[8]]+[\"0\"]+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "source910.close()\n",
    "result910.close()\n",
    "\n",
    "# failure2\n",
    "print('failure2 RUL determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv\", \"r\", newline='') as source920:\n",
    "    reader920 = csv.reader(source920)\n",
    "    with open(\"data/01_intermediateData/PdM_07_RUL20.csv\", \"w\", newline='') as result920:\n",
    "        writer520 = csv.writer(result920)\n",
    "        writeData = 0\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 0\n",
    "        for r920 in reader920:\n",
    "            if r920[0] == \"datetime\":\n",
    "                writer520.writerow(r920+[\"RUL\"])\n",
    "            else:\n",
    "                mID = int(r920[1])\n",
    "                if mID != mIDprev:\n",
    "                    writeData = 0\n",
    "                    mIDprev = mID\n",
    "                else:\n",
    "                    if r920[9] == \"20\":\n",
    "                        writeData = 1\n",
    "                        i = 0\n",
    "                        writer520.writerow(r920+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r920[9] == \"0\":\n",
    "                        writer520.writerow(r920+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r920[9] != \"0\" and r920[9] != \"20\":\n",
    "                        writer520.writerow([r920[0]]+[r920[1]]+[r920[2]]+[r920[3]]+[r920[4]]+[r920[5]]+[r920[6]]+[r920[7]]+[r920[8]]+[\"0\"]+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "source920.close()\n",
    "result920.close()\n",
    "\n",
    "# failure3\n",
    "print('failure3 RUL determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv\", \"r\", newline='') as source930:\n",
    "    reader930 = csv.reader(source930)\n",
    "    with open(\"data/01_intermediateData/PdM_07_RUL30.csv\", \"w\", newline='') as result930:\n",
    "        writer530 = csv.writer(result930)\n",
    "        writeData = 0\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 0\n",
    "        for r930 in reader930:\n",
    "            if r930[0] == \"datetime\":\n",
    "                writer530.writerow(r930+[\"RUL\"])\n",
    "            else:\n",
    "                mID = int(r930[1])\n",
    "                if mID != mIDprev:\n",
    "                    writeData = 0\n",
    "                    mIDprev = mID\n",
    "                else:\n",
    "                    if r930[9] == \"30\":\n",
    "                        writeData = 1\n",
    "                        i = 0\n",
    "                        writer530.writerow(r930+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r930[9] == \"0\":\n",
    "                        writer530.writerow(r930+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r930[9] != \"0\" and r930[9] != \"30\":\n",
    "                        writer530.writerow([r930[0]]+[r930[1]]+[r930[2]]+[r930[3]]+[r930[4]]+[r930[5]]+[r930[6]]+[r930[7]]+[r930[8]]+[\"0\"]+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "source930.close()\n",
    "result930.close()\n",
    "\n",
    "# failure4\n",
    "print('failure4 RUL determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_06_telemetry_model_age_maint_fail_final_sorted.csv\", \"r\", newline='') as source940:\n",
    "    reader940 = csv.reader(source940)\n",
    "    with open(\"data/01_intermediateData/PdM_07_RUL40.csv\", \"w\", newline='') as result940:\n",
    "        writer540 = csv.writer(result940)\n",
    "        writeData = 0\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 0\n",
    "        for r940 in reader940:\n",
    "            if r940[0] == \"datetime\":\n",
    "                writer540.writerow(r940+[\"RUL\"])\n",
    "            else:\n",
    "                mID = int(r940[1])\n",
    "                if mID != mIDprev:\n",
    "                    writeData = 0\n",
    "                    mIDprev = mID\n",
    "                else:\n",
    "                    if r940[9] == \"40\":\n",
    "                        writeData = 1\n",
    "                        i = 0\n",
    "                        writer540.writerow(r940+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r940[9] == \"0\":\n",
    "                        writer540.writerow(r940+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1 and r940[9] != \"0\" and r940[9] != \"40\":\n",
    "                        writer540.writerow([r940[0]]+[r940[1]]+[r940[2]]+[r940[3]]+[r940[4]]+[r940[5]]+[r940[6]]+[r940[7]]+[r940[8]]+[\"0\"]+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "source940.close()\n",
    "result940.close()\n",
    "\n",
    "print('Writing of .../data/01_intermediateData/PdM_07_RULn.csv files completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting data is in progress...\n",
      "Writing of .../data/01_intermediateData/PdM_08_RULn_sorted_asc.csv files completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sorting intermediate file by machineID (ascending) and datetime (ascending)\n",
    "print('Sorting data is in progress...')\n",
    "\n",
    "# failure1\n",
    "df310 = pd.read_csv('./data/01_intermediateData/PdM_07_RUL10.csv', delimiter=',')\n",
    "df310 = df310.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "df310.to_csv('./data/01_intermediateData/PdM_08_RUL10_sorted_asc.csv',index=False)\n",
    "# failure2\n",
    "df320 = pd.read_csv('./data/01_intermediateData/PdM_07_RUL20.csv', delimiter=',')\n",
    "df320 = df320.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "df320.to_csv('./data/01_intermediateData/PdM_08_RUL20_sorted_asc.csv',index=False)\n",
    "# failure3\n",
    "df330 = pd.read_csv('./data/01_intermediateData/PdM_07_RUL30.csv', delimiter=',')\n",
    "df330 = df330.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "df330.to_csv('./data/01_intermediateData/PdM_08_RUL30_sorted_asc.csv',index=False)\n",
    "# failure4\n",
    "df340 = pd.read_csv('./data/01_intermediateData/PdM_07_RUL40.csv', delimiter=',')\n",
    "df340 = df340.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "df340.to_csv('./data/01_intermediateData/PdM_08_RUL40_sorted_asc.csv',index=False)\n",
    "\n",
    "print('Writing of .../data/01_intermediateData/PdM_08_RULn_sorted_asc.csv files completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine cycle is in progress...\n",
      "failure1 cycle determination is in progress...\n",
      "failure2 cycle determination is in progress...\n",
      "failure3 cycle determination is in progress...\n",
      "failure4 cycle determination is in progress...\n",
      "Writing of .../data/01_intermediateData/PdM_09_RUL_cycle_n.csv files completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Determine cycle\n",
    "print('Determine cycle is in progress...')\n",
    "\n",
    "# failure1\n",
    "print('failure1 cycle determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_08_RUL10_sorted_asc.csv\", \"r\", newline='') as source410:\n",
    "    reader410 = csv.reader(source410)\n",
    "    with open(\"data/01_intermediateData/PdM_09_RUL_cycle_10.csv\", \"w\", newline='') as result410:\n",
    "        writer410 = csv.writer(result410)\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 1\n",
    "        prevRUL = 0\n",
    "        for r410 in reader410:\n",
    "            if r410[0] == \"datetime\":\n",
    "                writer410.writerow((r410[0], r410[1], str('cycle'), r410[2], r410[3], r410[4], r410[5], r410[6], r410[7], r410[8], r410[9], r410[10]))\n",
    "            else:\n",
    "                mID = int(r410[1])\n",
    "                if mID != mIDprev:\n",
    "                    i = 1\n",
    "                    writer410.writerow((r410[0], r410[1], str(i), r410[2][-1:], r410[3], r410[4], r410[5], r410[6], r410[7], r410[8], r410[9], r410[10]))\n",
    "                    mIDprev = mID\n",
    "                    i = i+1\n",
    "                    prevRUL = r410[10]\n",
    "                else:\n",
    "                    if r410[9] != str(0):\n",
    "                        writer410.writerow((r410[0], r410[1], str(i), r410[2][-1:], r410[3], r410[4], r410[5], r410[6], r410[7], r410[8], r410[9], r410[10]))\n",
    "                        prevRUL = r410[10]\n",
    "                    elif r410[9] == str(0) and prevRUL == str(0):\n",
    "                        i = 1\n",
    "                        writer410.writerow((r410[0], r410[1], str(i), r410[2][-1:], r410[3], r410[4], r410[5], r410[6], r410[7], r410[8], r410[9], r410[10]))\n",
    "                        prevRUL = r410[10]\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        writer410.writerow((r410[0], r410[1], str(i), r410[2][-1:], r410[3], r410[4], r410[5], r410[6], r410[7], r410[8], r410[9], r410[10]))\n",
    "                        i = i+1\n",
    "                        prevRUL = r410[10]\n",
    "source410.close()\n",
    "result410.close()\n",
    "\n",
    "# failure2\n",
    "print('failure2 cycle determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_08_RUL20_sorted_asc.csv\", \"r\", newline='') as source420:\n",
    "    reader420 = csv.reader(source420)\n",
    "    with open(\"data/01_intermediateData/PdM_09_RUL_cycle_20.csv\", \"w\", newline='') as result420:\n",
    "        writer420 = csv.writer(result420)\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 1\n",
    "        prevRUL = 0\n",
    "        for r420 in reader420:\n",
    "            if r420[0] == \"datetime\":\n",
    "                writer420.writerow((r420[0], r420[1], str('cycle'), r420[2], r420[3], r420[4], r420[5], r420[6], r420[7], r420[8], r420[9], r420[10]))\n",
    "            else:\n",
    "                mID = int(r420[1])\n",
    "                if mID != mIDprev:\n",
    "                    i = 1\n",
    "                    writer420.writerow((r420[0], r420[1], str(i), r420[2][-1:], r420[3], r420[4], r420[5], r420[6], r420[7], r420[8], r420[9], r420[10]))\n",
    "                    mIDprev = mID\n",
    "                    i = i+1\n",
    "                    prevRUL = r420[10]\n",
    "                else:\n",
    "                    if r420[9] != str(0):\n",
    "                        writer420.writerow((r420[0], r420[1], str(i), r420[2][-1:], r420[3], r420[4], r420[5], r420[6], r420[7], r420[8], r420[9], r420[10]))\n",
    "                        prevRUL = r420[10]\n",
    "                    elif r420[9] == str(0) and prevRUL == str(0):\n",
    "                        i = 1\n",
    "                        writer420.writerow((r420[0], r420[1], str(i), r420[2][-1:], r420[3], r420[4], r420[5], r420[6], r420[7], r420[8], r420[9], r420[10]))\n",
    "                        prevRUL = r420[10]\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        writer420.writerow((r420[0], r420[1], str(i), r420[2][-1:], r420[3], r420[4], r420[5], r420[6], r420[7], r420[8], r420[9], r420[10]))\n",
    "                        i = i+1\n",
    "                        prevRUL = r420[10]\n",
    "source420.close()\n",
    "result420.close()\n",
    "\n",
    "# failure3\n",
    "print('failure3 cycle determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_08_RUL30_sorted_asc.csv\", \"r\", newline='') as source430:\n",
    "    reader430 = csv.reader(source430)\n",
    "    with open(\"data/01_intermediateData/PdM_09_RUL_cycle_30.csv\", \"w\", newline='') as result430:\n",
    "        writer430 = csv.writer(result430)\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 1\n",
    "        prevRUL = 0\n",
    "        for r430 in reader430:\n",
    "            if r430[0] == \"datetime\":\n",
    "                writer430.writerow((r430[0], r430[1], str('cycle'), r430[2], r430[3], r430[4], r430[5], r430[6], r430[7], r430[8], r430[9], r430[10]))\n",
    "            else:\n",
    "                mID = int(r430[1])\n",
    "                if mID != mIDprev:\n",
    "                    i = 1\n",
    "                    writer430.writerow((r430[0], r430[1], str(i), r430[2][-1:], r430[3], r430[4], r430[5], r430[6], r430[7], r430[8], r430[9], r430[10]))\n",
    "                    mIDprev = mID\n",
    "                    i = i+1\n",
    "                    prevRUL = r430[10]\n",
    "                else:\n",
    "                    if r430[9] != str(0):\n",
    "                        writer430.writerow((r430[0], r430[1], str(i), r430[2][-1:], r430[3], r430[4], r430[5], r430[6], r430[7], r430[8], r430[9], r430[10]))\n",
    "                        prevRUL = r430[10]\n",
    "                    elif r430[9] == str(0) and prevRUL == str(0):\n",
    "                        i = 1\n",
    "                        writer430.writerow((r430[0], r430[1], str(i), r430[2][-1:], r430[3], r430[4], r430[5], r430[6], r430[7], r430[8], r430[9], r430[10]))\n",
    "                        prevRUL = r430[10]\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        writer430.writerow((r430[0], r430[1], str(i), r430[2][-1:], r430[3], r430[4], r430[5], r430[6], r430[7], r430[8], r430[9], r430[10]))\n",
    "                        i = i+1\n",
    "                        prevRUL = r430[10]\n",
    "source430.close()\n",
    "result430.close()\n",
    "\n",
    "# failure4\n",
    "print('failure4 cycle determination is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_08_RUL40_sorted_asc.csv\", \"r\", newline='') as source440:\n",
    "    reader440 = csv.reader(source440)\n",
    "    with open(\"data/01_intermediateData/PdM_09_RUL_cycle_40.csv\", \"w\", newline='') as result440:\n",
    "        writer440 = csv.writer(result440)\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 1\n",
    "        prevRUL = 0\n",
    "        for r440 in reader440:\n",
    "            if r440[0] == \"datetime\":\n",
    "                writer440.writerow((r440[0], r440[1], str('cycle'), r440[2], r440[3], r440[4], r440[5], r440[6], r440[7], r440[8], r440[9], r440[10]))\n",
    "            else:\n",
    "                mID = int(r440[1])\n",
    "                if mID != mIDprev:\n",
    "                    i = 1\n",
    "                    writer440.writerow((r440[0], r440[1], str(i), r440[2][-1:], r440[3], r440[4], r440[5], r440[6], r440[7], r440[8], r440[9], r440[10]))\n",
    "                    mIDprev = mID\n",
    "                    i = i+1\n",
    "                    prevRUL = r440[10]\n",
    "                else:\n",
    "                    if r440[9] != str(0):\n",
    "                        writer440.writerow((r440[0], r440[1], str(i), r440[2][-1:], r440[3], r440[4], r440[5], r440[6], r440[7], r440[8], r440[9], r440[10]))\n",
    "                        prevRUL = r440[10]\n",
    "                    elif r440[9] == str(0) and prevRUL == str(0):\n",
    "                        i = 1\n",
    "                        writer440.writerow((r440[0], r440[1], str(i), r440[2][-1:], r440[3], r440[4], r440[5], r440[6], r440[7], r440[8], r440[9], r440[10]))\n",
    "                        prevRUL = r440[10]\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        writer440.writerow((r440[0], r440[1], str(i), r440[2][-1:], r440[3], r440[4], r440[5], r440[6], r440[7], r440[8], r440[9], r440[10]))\n",
    "                        i = i+1\n",
    "                        prevRUL = r440[10]\n",
    "source440.close()\n",
    "result440.close()\n",
    "\n",
    "print('Writing of .../data/01_intermediateData/PdM_09_RUL_cycle_n.csv files completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into train and test data is in progress...\n",
      "Writing train data for failure1 is in progress...\n",
      "Writing train data for failure2 is in progress...\n",
      "Writing train data for failure3 is in progress...\n",
      "Writing train data for failure4 is in progress...\n",
      "Writing test data for failure1 is in progress...\n",
      "Writing test data for failure2 is in progress...\n",
      "Writing test data for failure3 is in progress...\n",
      "Writing test data for failure4 is in progress...\n",
      "Split data into train and test data completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test data\n",
    "print('Split data into train and test data is in progress...')\n",
    "\n",
    "# write train data for failure1\n",
    "print('Writing train data for failure1 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_10.csv\", \"r\", newline='') as source510:\n",
    "    reader510 = csv.reader(source510)\n",
    "    with open(\"data/01_intermediateData/PdM_10_train_10.csv\", \"w\", newline='') as result510:\n",
    "        writer510 = csv.writer(result510)\n",
    "        for r510 in reader510:\n",
    "            if r510[1] not in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer510.writerow(r510)\n",
    "source510.close()\n",
    "result510.close()\n",
    "\n",
    "# write train data for failure2\n",
    "print('Writing train data for failure2 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_20.csv\", \"r\", newline='') as source520:\n",
    "    reader520 = csv.reader(source520)\n",
    "    with open(\"data/01_intermediateData/PdM_10_train_20.csv\", \"w\", newline='') as result520:\n",
    "        writer520 = csv.writer(result520)\n",
    "        for r520 in reader520:\n",
    "            if r520[1] not in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer520.writerow(r520)\n",
    "source520.close()\n",
    "result520.close()\n",
    "\n",
    "# write train data for failure3\n",
    "print('Writing train data for failure3 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_30.csv\", \"r\", newline='') as source530:\n",
    "    reader530 = csv.reader(source530)\n",
    "    with open(\"data/01_intermediateData/PdM_10_train_30.csv\", \"w\", newline='') as result530:\n",
    "        writer530 = csv.writer(result530)\n",
    "        for r530 in reader530:\n",
    "            if r530[1] not in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer530.writerow(r530)\n",
    "source530.close()\n",
    "result530.close()\n",
    "\n",
    "# write train data for failure4\n",
    "print('Writing train data for failure4 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_40.csv\", \"r\", newline='') as source540:\n",
    "    reader540 = csv.reader(source540)\n",
    "    with open(\"data/01_intermediateData/PdM_10_train_40.csv\", \"w\", newline='') as result540:\n",
    "        writer540 = csv.writer(result540)\n",
    "        for r540 in reader540:\n",
    "            if r540[1] not in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer540.writerow(r540)\n",
    "source540.close()\n",
    "result540.close()\n",
    "\n",
    "# write test data for failure1\n",
    "print('Writing test data for failure1 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_10.csv\", \"r\", newline='') as source610:\n",
    "    reader610 = csv.reader(source610)\n",
    "    with open(\"data/01_intermediateData/PdM_10_test_10.csv\", \"w\", newline='') as result610:\n",
    "        writer610 = csv.writer(result610)\n",
    "        for r610 in reader610:\n",
    "            if r610[0] == \"datetime\" or r610[1] in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer610.writerow(r610)\n",
    "source610.close()\n",
    "result610.close()\n",
    "\n",
    "# write test data for failure2\n",
    "print('Writing test data for failure2 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_20.csv\", \"r\", newline='') as source620:\n",
    "    reader620 = csv.reader(source620)\n",
    "    with open(\"data/01_intermediateData/PdM_10_test_20.csv\", \"w\", newline='') as result620:\n",
    "        writer620 = csv.writer(result620)\n",
    "        for r620 in reader620:\n",
    "            if r620[0] == \"datetime\" or r620[1] in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer620.writerow(r620)\n",
    "source620.close()\n",
    "result620.close()\n",
    "\n",
    "# write test data for failure3\n",
    "print('Writing test data for failure3 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_30.csv\", \"r\", newline='') as source630:\n",
    "    reader630 = csv.reader(source630)\n",
    "    with open(\"data/01_intermediateData/PdM_10_test_30.csv\", \"w\", newline='') as result630:\n",
    "        writer630 = csv.writer(result630)\n",
    "        for r630 in reader630:\n",
    "            if r630[0] == \"datetime\" or r630[1] in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer630.writerow(r630)\n",
    "source630.close()\n",
    "result630.close()\n",
    "\n",
    "# write test data for failure4\n",
    "print('Writing test data for failure4 is in progress...')\n",
    "with open(\"data/01_intermediateData/PdM_09_RUL_cycle_40.csv\", \"r\", newline='') as source640:\n",
    "    reader640 = csv.reader(source640)\n",
    "    with open(\"data/01_intermediateData/PdM_10_test_40.csv\", \"w\", newline='') as result640:\n",
    "        writer640 = csv.writer(result640)\n",
    "        for r640 in reader640:\n",
    "            if r640[0] == \"datetime\" or r640[1] in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer640.writerow(r640)\n",
    "source640.close()\n",
    "result640.close()\n",
    "\n",
    "print('Split data into train and test data completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data \n",
    "train_df10 = pd.read_csv('./data/01_intermediateData/PdM_10_train_10.csv', sep=\",\", header=0)\n",
    "train_df20 = pd.read_csv('./data/01_intermediateData/PdM_10_train_20.csv', sep=\",\", header=0)\n",
    "train_df30 = pd.read_csv('./data/01_intermediateData/PdM_10_train_30.csv', sep=\",\", header=0)\n",
    "train_df40 = pd.read_csv('./data/01_intermediateData/PdM_10_train_40.csv', sep=\",\", header=0)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_df10 = pd.read_csv('./data/01_intermediateData/PdM_10_test_10.csv', sep=\",\", header=0)\n",
    "test_df20 = pd.read_csv('./data/01_intermediateData/PdM_10_test_20.csv', sep=\",\", header=0)\n",
    "test_df30 = pd.read_csv('./data/01_intermediateData/PdM_10_test_30.csv', sep=\",\", header=0)\n",
    "test_df40 = pd.read_csv('./data/01_intermediateData/PdM_10_test_40.csv', sep=\",\", header=0)\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate label columns for training data\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "\n",
    "# Label1 indicates a failure will occur within the next 30 cycles.\n",
    "# 1 indicates failure, 0 indicates healthy \n",
    "train_df10['label1'] = np.where(train_df10['RUL'] <= w1, 1, 0 )\n",
    "train_df20['label1'] = np.where(train_df20['RUL'] <= w1, 1, 0 )\n",
    "train_df30['label1'] = np.where(train_df30['RUL'] <= w1, 1, 0 )\n",
    "train_df40['label1'] = np.where(train_df40['RUL'] <= w1, 1, 0 )\n",
    "\n",
    "# label2 is multiclass, value 1 is identical to label1,\n",
    "# value 2 indicates failure within 15 cycles\n",
    "train_df10['label2'] = train_df10['label1']\n",
    "train_df10.loc[train_df10['RUL'] <= w0, 'label2'] = 2\n",
    "train_df20['label2'] = train_df20['label1']\n",
    "train_df20.loc[train_df20['RUL'] <= w0, 'label2'] = 2\n",
    "train_df30['label2'] = train_df30['label1']\n",
    "train_df30.loc[train_df30['RUL'] <= w0, 'label2'] = 2\n",
    "train_df40['label2'] = train_df40['label1']\n",
    "train_df40.loc[train_df40['RUL'] <= w0, 'label2'] = 2\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - train - step 1\n",
    "train_df10['cycle_norm'] = train_df10['cycle']\n",
    "train_df10.drop(train_df10.columns[[9]], axis=1, inplace=True)\n",
    "#train_df10.drop(train_df10.columns[[9]], axis=1, inplace=True)\n",
    "train_df20['cycle_norm'] = train_df20['cycle']\n",
    "train_df20.drop(train_df20.columns[[9]], axis=1, inplace=True)\n",
    "#train_df20.drop(train_df20.columns[[9]], axis=1, inplace=True)\n",
    "train_df30['cycle_norm'] = train_df30['cycle']\n",
    "train_df30.drop(train_df30.columns[[9]], axis=1, inplace=True)\n",
    "#train_df30.drop(train_df30.columns[[9]], axis=1, inplace=True)\n",
    "train_df40['cycle_norm'] = train_df40['cycle']\n",
    "train_df40.drop(train_df40.columns[[9]], axis=1, inplace=True)\n",
    "#train_df40.drop(train_df40.columns[[9]], axis=1, inplace=True)\n",
    "\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization - train - step 2\n",
    "# failure1\n",
    "train_df_datetime10 = train_df10['datetime']\n",
    "train_df10 = train_df10[['machineID','cycle','model','age','volt','rotate','pressure','vibration','failures','RUL','label1','label2','cycle_norm']]\n",
    "cols_normalize10 = train_df10.columns.difference(['machineID','cycle','failures','RUL','label1','label2'])\n",
    "min_max_scaler10 = MinMaxScaler()\n",
    "norm_train_df10 = pd.DataFrame(min_max_scaler10.fit_transform(train_df10[cols_normalize10]), \n",
    "                             columns=cols_normalize10, \n",
    "                             index=train_df10.index)\n",
    "join_df10 = train_df10[train_df10.columns.difference(cols_normalize10)].join(norm_train_df10)\n",
    "train_df10 = join_df10.reindex(columns = train_df10.columns)\n",
    "train_df10 = pd.concat([train_df_datetime10, train_df10],axis=1)\n",
    "\n",
    "# failure2\n",
    "train_df_datetime20 = train_df20['datetime']\n",
    "train_df20 = train_df20[['machineID','cycle','model','age','volt','rotate','pressure','vibration','failures','RUL','label1','label2','cycle_norm']]\n",
    "cols_normalize20 = train_df20.columns.difference(['machineID','cycle','failures','RUL','label1','label2'])\n",
    "min_max_scaler20 = MinMaxScaler()\n",
    "norm_train_df20 = pd.DataFrame(min_max_scaler20.fit_transform(train_df20[cols_normalize20]), \n",
    "                             columns=cols_normalize20, \n",
    "                             index=train_df20.index)\n",
    "join_df20 = train_df20[train_df20.columns.difference(cols_normalize20)].join(norm_train_df20)\n",
    "train_df20 = join_df20.reindex(columns = train_df20.columns)\n",
    "train_df20 = pd.concat([train_df_datetime20, train_df20],axis=1)\n",
    "\n",
    "# failure3\n",
    "train_df_datetime30 = train_df30['datetime']\n",
    "train_df30 = train_df30[['machineID','cycle','model','age','volt','rotate','pressure','vibration','failures','RUL','label1','label2','cycle_norm']]\n",
    "cols_normalize30 = train_df30.columns.difference(['machineID','cycle','failures','RUL','label1','label2'])\n",
    "min_max_scaler30 = MinMaxScaler()\n",
    "norm_train_df30 = pd.DataFrame(min_max_scaler30.fit_transform(train_df30[cols_normalize30]), \n",
    "                             columns=cols_normalize30, \n",
    "                             index=train_df30.index)\n",
    "join_df30 = train_df30[train_df30.columns.difference(cols_normalize30)].join(norm_train_df30)\n",
    "train_df30 = join_df30.reindex(columns = train_df30.columns)\n",
    "train_df30 = pd.concat([train_df_datetime30, train_df30],axis=1)\n",
    "\n",
    "# failure4\n",
    "train_df_datetime40 = train_df40['datetime']\n",
    "train_df40 = train_df40[['machineID','cycle','model','age','volt','rotate','pressure','vibration','failures','RUL','label1','label2','cycle_norm']]\n",
    "cols_normalize40 = train_df40.columns.difference(['machineID','cycle','failures','RUL','label1','label2'])\n",
    "min_max_scaler40 = MinMaxScaler()\n",
    "norm_train_df40 = pd.DataFrame(min_max_scaler40.fit_transform(train_df40[cols_normalize40]), \n",
    "                             columns=cols_normalize40, \n",
    "                             index=train_df40.index)\n",
    "join_df40 = train_df40[train_df40.columns.difference(cols_normalize40)].join(norm_train_df40)\n",
    "train_df40 = join_df40.reindex(columns = train_df40.columns)\n",
    "train_df40 = pd.concat([train_df_datetime40, train_df40],axis=1)\n",
    "\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - test - step 1\n",
    "# failure1\n",
    "test_df10['cycle_norm'] = test_df10['cycle']\n",
    "test_df10.drop(test_df10.columns[[0]], axis=1, inplace=True)\n",
    "test_df10.drop(test_df10.columns[[8]], axis=1, inplace=True)\n",
    "test_df10.drop(test_df10.columns[[8]], axis=1, inplace=True)\n",
    "\n",
    "# failure2\n",
    "test_df20['cycle_norm'] = test_df20['cycle']\n",
    "test_df20.drop(test_df20.columns[[0]], axis=1, inplace=True)\n",
    "test_df20.drop(test_df20.columns[[8]], axis=1, inplace=True)\n",
    "test_df20.drop(test_df20.columns[[8]], axis=1, inplace=True)\n",
    "\n",
    "# failure3\n",
    "test_df30['cycle_norm'] = test_df30['cycle']\n",
    "test_df30.drop(test_df30.columns[[0]], axis=1, inplace=True)\n",
    "test_df30.drop(test_df30.columns[[8]], axis=1, inplace=True)\n",
    "test_df30.drop(test_df30.columns[[8]], axis=1, inplace=True)\n",
    "\n",
    "# failure4\n",
    "test_df40['cycle_norm'] = test_df40['cycle']\n",
    "test_df40.drop(test_df40.columns[[0]], axis=1, inplace=True)\n",
    "test_df40.drop(test_df40.columns[[8]], axis=1, inplace=True)\n",
    "test_df40.drop(test_df40.columns[[8]], axis=1, inplace=True)\n",
    "\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - test - step 2\n",
    "# failure1\n",
    "cols_normalize10 = test_df10.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler10 = MinMaxScaler()\n",
    "norm_test_df10 = pd.DataFrame(min_max_scaler10.fit_transform(test_df10[cols_normalize10]), \n",
    "                             columns=cols_normalize10, \n",
    "                             index=test_df10.index)\n",
    "join_df10 = test_df10[test_df10.columns.difference(cols_normalize10)].join(norm_test_df10)\n",
    "test_df10 = join_df10.reindex(columns = test_df10.columns)\n",
    "\n",
    "# failure2\n",
    "cols_normalize20 = test_df20.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler20 = MinMaxScaler()\n",
    "norm_test_df20 = pd.DataFrame(min_max_scaler20.fit_transform(test_df20[cols_normalize20]), \n",
    "                             columns=cols_normalize20, \n",
    "                             index=test_df20.index)\n",
    "join_df20 = test_df20[test_df20.columns.difference(cols_normalize20)].join(norm_test_df20)\n",
    "test_df20 = join_df20.reindex(columns = test_df20.columns)\n",
    "\n",
    "# failure3\n",
    "cols_normalize30 = test_df30.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler30 = MinMaxScaler()\n",
    "norm_test_df30 = pd.DataFrame(min_max_scaler30.fit_transform(test_df30[cols_normalize30]), \n",
    "                             columns=cols_normalize30, \n",
    "                             index=test_df30.index)\n",
    "join_df30 = test_df30[test_df30.columns.difference(cols_normalize30)].join(norm_test_df30)\n",
    "test_df30 = join_df30.reindex(columns = test_df30.columns)\n",
    "\n",
    "# failure4\n",
    "cols_normalize40 = test_df40.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler40 = MinMaxScaler()\n",
    "norm_test_df40 = pd.DataFrame(min_max_scaler40.fit_transform(test_df40[cols_normalize40]), \n",
    "                             columns=cols_normalize40, \n",
    "                             index=test_df40.index)\n",
    "join_df40 = test_df40[test_df40.columns.difference(cols_normalize40)].join(norm_test_df40)\n",
    "test_df40 = join_df40.reindex(columns = test_df40.columns)\n",
    "\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label columns w0 and w1 for test data\n",
    "# failure1\n",
    "test_df10['label1'] = np.where(test_df10['RUL'] <= w1, 1, 0 )\n",
    "test_df10['label2'] = test_df10['label1']\n",
    "test_df10.loc[test_df10['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# failure2\n",
    "test_df20['label1'] = np.where(test_df20['RUL'] <= w1, 1, 0 )\n",
    "test_df20['label2'] = test_df20['label1']\n",
    "test_df20.loc[test_df20['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# failure3\n",
    "test_df30['label1'] = np.where(test_df30['RUL'] <= w1, 1, 0 )\n",
    "test_df30['label2'] = test_df30['label1']\n",
    "test_df30.loc[test_df30['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# failure4\n",
    "test_df40['label1'] = np.where(test_df40['RUL'] <= w1, 1, 0 )\n",
    "test_df40['label2'] = test_df40['label1']\n",
    "test_df40.loc[test_df40['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work\\ML\\predictive_maintenance\\work_errortypepredictors\\1_data_ingestion_and_preparation.ipynb Cell 28\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/ML/predictive_maintenance/work_errortypepredictors/1_data_ingestion_and_preparation.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m engine_id3_50cycleWindow2 \u001b[39m=\u001b[39m engine_id3_50cycleWindow[cols2]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Work/ML/predictive_maintenance/work_errortypepredictors/1_data_ingestion_and_preparation.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# plotting sensor data for engine ID 89 prior to a failure point - sensors 1-2 \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Work/ML/predictive_maintenance/work_errortypepredictors/1_data_ingestion_and_preparation.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ax1 \u001b[39m=\u001b[39m engine_id3_50cycleWindow1\u001b[39m.\u001b[39;49mplot(subplots\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sharex\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, figsize\u001b[39m=\u001b[39;49m(\u001b[39m20\u001b[39;49m,\u001b[39m20\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\plotting\\_core.py:794\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m             label_name \u001b[39m=\u001b[39m label_kw \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m    792\u001b[0m             data\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m label_name\n\u001b[1;32m--> 794\u001b[0m \u001b[39mreturn\u001b[39;00m plot_backend\u001b[39m.\u001b[39;49mplot(data, kind\u001b[39m=\u001b[39;49mkind, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:62\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39max\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(ax, \u001b[39m\"\u001b[39m\u001b[39mleft_ax\u001b[39m\u001b[39m\"\u001b[39m, ax)\n\u001b[0;32m     61\u001b[0m plot_obj \u001b[39m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 62\u001b[0m plot_obj\u001b[39m.\u001b[39;49mgenerate()\n\u001b[0;32m     63\u001b[0m plot_obj\u001b[39m.\u001b[39mdraw()\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m plot_obj\u001b[39m.\u001b[39mresult\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:279\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_adjust()\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_plot_data()\n\u001b[0;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_subplots()\n\u001b[0;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_plot()\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:403\u001b[0m, in \u001b[0;36mMPLPlot._compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39m# GH16953, _convert is needed as fallback, for ``Series``\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m# with ``dtype == object``\u001b[39;00m\n\u001b[0;32m    402\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39m_convert(datetime\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, timedelta\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 403\u001b[0m numeric_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mselect_dtypes(\n\u001b[0;32m    404\u001b[0m     include\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mnumber, \u001b[39m\"\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdatetimetz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtimedelta\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    405\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     is_empty \u001b[39m=\u001b[39m numeric_data\u001b[39m.\u001b[39mempty\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3442\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   3425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minclude and exclude overlap on \u001b[39m\u001b[39m{inc_ex}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   3427\u001b[0m             inc_ex\u001b[39m=\u001b[39m(include \u001b[39m&\u001b[39m exclude)\n\u001b[0;32m   3428\u001b[0m         )\n\u001b[0;32m   3429\u001b[0m     )\n\u001b[0;32m   3431\u001b[0m \u001b[39m# empty include/exclude -> defaults to True\u001b[39;00m\n\u001b[0;32m   3432\u001b[0m \u001b[39m# three cases (we've already raised if both are empty)\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[39m# case 1: empty include, nonempty exclude\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3440\u001b[0m \u001b[39m# the \"union\" of the logic of case 1 and case 2:\u001b[39;00m\n\u001b[0;32m   3441\u001b[0m \u001b[39m# we get the included and excluded, and return their logical and\u001b[39;00m\n\u001b[1;32m-> 3442\u001b[0m include_these \u001b[39m=\u001b[39m Series(\u001b[39mnot\u001b[39;49;00m \u001b[39mbool\u001b[39;49m(include), index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns)\n\u001b[0;32m   3443\u001b[0m exclude_these \u001b[39m=\u001b[39m Series(\u001b[39mnot\u001b[39;00m \u001b[39mbool\u001b[39m(exclude), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m   3445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_dtype_instance_mapper\u001b[39m(idx, dtype):\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\series.py:314\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    312\u001b[0m             data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    313\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m         data \u001b[39m=\u001b[39m sanitize_array(data, index, dtype, copy, raise_cast_failure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    316\u001b[0m         data \u001b[39m=\u001b[39m SingleBlockManager(data, index, fastpath\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    318\u001b[0m generic\u001b[39m.\u001b[39mNDFrame\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, fastpath\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\construction.py:712\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m         \u001b[39m# need to possibly convert the value here\u001b[39;00m\n\u001b[0;32m    710\u001b[0m         value \u001b[39m=\u001b[39m maybe_cast_to_datetime(value, dtype)\n\u001b[1;32m--> 712\u001b[0m     subarr \u001b[39m=\u001b[39m construct_1d_arraylike_from_scalar(value, \u001b[39mlen\u001b[39;49m(index), dtype)\n\u001b[0;32m    714\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[39mreturn\u001b[39;00m subarr\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bordasm\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1233\u001b[0m, in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[1;34m(value, length, dtype)\u001b[0m\n\u001b[0;32m   1230\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isna(value):\n\u001b[0;32m   1231\u001b[0m             value \u001b[39m=\u001b[39m ensure_str(value)\n\u001b[1;32m-> 1233\u001b[0m     subarr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mempty(length, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1234\u001b[0m     subarr\u001b[39m.\u001b[39mfill(value)\n\u001b[0;32m   1236\u001b[0m \u001b[39mreturn\u001b[39;00m subarr\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "# preparing data for visualizations \n",
    "# window of 50 cycles prior to a failure point for engine id 89\n",
    "engine_id3 = test_df10[test_df10['machineID'] == 89]\n",
    "#engine_id3 = test_df20[test_df20['machineID'] == 89]\n",
    "#engine_id3 = test_df30[test_df30['machineID'] == 89]\n",
    "#engine_id3 = test_df40[test_df40['machineID'] == 89]\n",
    "engine_id3_50cycleWindow = engine_id3[engine_id3['RUL'] <= engine_id3['RUL'].min() + 50]\n",
    "cols1 = ['volt', 'rotate']\n",
    "engine_id3_50cycleWindow1 = engine_id3_50cycleWindow[cols1]\n",
    "cols2 = ['pressure', 'vibration']\n",
    "engine_id3_50cycleWindow2 = engine_id3_50cycleWindow[cols2]\n",
    "\n",
    "# plotting sensor data for engine ID 89 prior to a failure point - sensors 1-2 \n",
    "ax1 = engine_id3_50cycleWindow1.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting sensor data for engine ID 89 prior to a failure point - sensors 3-4 \n",
    "ax2 = engine_id3_50cycleWindow2.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving into files in progress...\n",
      "Data files saved!\n"
     ]
    }
   ],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert store it for later manipulations in subsequent notebooks.\n",
    "print('Data saving into files in progress...')\n",
    "\n",
    "train_df10.to_csv('./data/02_preparedData/PdM_train_prepared_failure1.csv', encoding='utf-8', index=False)\n",
    "train_df20.to_csv('./data/02_preparedData/PdM_train_prepared_failure2.csv', encoding='utf-8', index=False)\n",
    "train_df30.to_csv('./data/02_preparedData/PdM_train_prepared_failure3.csv', encoding='utf-8', index=False)\n",
    "train_df40.to_csv('./data/02_preparedData/PdM_train_prepared_failure4.csv', encoding='utf-8', index=False)\n",
    "\n",
    "test_df10.to_csv('./data/02_preparedData/PdM_test_prepared_failure1.csv', encoding='utf-8', index=False)\n",
    "test_df20.to_csv('./data/02_preparedData/PdM_test_prepared_failure2.csv', encoding='utf-8', index=False)\n",
    "test_df30.to_csv('./data/02_preparedData/PdM_test_prepared_failure3.csv', encoding='utf-8', index=False)\n",
    "test_df40.to_csv('./data/02_preparedData/PdM_test_prepared_failure4.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(\"Data files saved!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "f1edc3b7e2bc1b056dd3afabe23c0a81e5f58df78ee98050cdc1bb46216e89a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
