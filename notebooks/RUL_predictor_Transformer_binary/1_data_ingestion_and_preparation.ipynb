{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of this notebook: https://github.com/Azure-Samples/MachineLearningSamples-DeepLearningforPredictiveMaintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Ingestion & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "print(\"Importing the libraries is in progress...\")\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import urllib\n",
    "\n",
    "print(\"Import complete successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source of dataset: https://www.kaggle.com/arnabbiswas1/microsoft-azure-predictive-maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting input file by machineID (ascending) and datetime (descending) to determine RUL\n",
    "print('Sorting data is in progress...')\n",
    "\n",
    "df1 = pd.read_csv('./data/sourceData/PdM_telemetry_model_age_07_maint_err_fail_final.csv', delimiter=',')\n",
    "df1 = df1.sort_values(['machineID', 'datetime'], ascending=[True, False])\n",
    "df1.to_csv('./data/intermediateData/01_PdM_telemetry_model_age_07_maint_err_fail_final_sorted_pre.csv')\n",
    "\n",
    "with open(\"data/intermediateData/01_PdM_telemetry_model_age_07_maint_err_fail_final_sorted_pre.csv\", \"r\", newline='') as source1:\n",
    "    reader1 = csv.reader(source1)\n",
    "      \n",
    "    with open(\"data/intermediateData/02_PdM_telemetry_model_age_07_maint_err_fail_final_sorted.csv\", \"w\", newline='') as result1:\n",
    "        writer1 = csv.writer(result1)\n",
    "        for r1 in reader1:\n",
    "            writer1.writerow((r1[1], r1[2], r1[3], r1[4], r1[5], r1[6], r1[7], r1[8], r1[9], r1[10]))\n",
    "\n",
    "source1.close()\n",
    "result1.close()\n",
    "\n",
    "print('Writing of .../data/intermediateData/02_PdM_telemetry_model_age_07_maint_err_fail_final_sorted.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine RUL\n",
    "print('Determine RUL is in progress...')\n",
    "\n",
    "with open(\"data/intermediateData/02_PdM_telemetry_model_age_07_maint_err_fail_final_sorted.csv\", \"r\", newline='') as source2:\n",
    "    reader2 = csv.reader(source2)\n",
    "      \n",
    "    with open(\"data/intermediateData/03_PdM_telemetry_model_age_07_maint_err_fail_final_RUL.csv\", \"w\", newline='') as result2:\n",
    "        writer2 = csv.writer(result2)\n",
    "        writeData = 0\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 0\n",
    "        for r2 in reader2:\n",
    "            if r2[0] == \"datetime\":\n",
    "                writer2.writerow(r2+[\"RUL\"])\n",
    "            else:\n",
    "                mID = int(r2[1])\n",
    "                if mID != mIDprev:\n",
    "                    writeData = 0\n",
    "                    mIDprev = mID\n",
    "                else:\n",
    "                    if ('comp' in r2[9]) == True:\n",
    "                        writeData = 1\n",
    "                        i = 0\n",
    "                        writer2.writerow(r2+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "                    elif writeData == 1:\n",
    "                        writer2.writerow(r2+[i])\n",
    "                        mIDprev = mID\n",
    "                        i = i+1\n",
    "\n",
    "source2.close()\n",
    "result2.close()\n",
    "\n",
    "print('Writing of .../data/intermediateData/03_PdM_telemetry_model_age_07_maint_err_fail_final_RUL.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting intermediate file by machineID (ascending) and datetime (ascending)\n",
    "print('Sorting data is in progress...')\n",
    "\n",
    "df3 = pd.read_csv('./data/intermediateData/03_PdM_telemetry_model_age_07_maint_err_fail_final_RUL.csv', delimiter=',')\n",
    "df3 = df3.sort_values(['machineID', 'datetime'], ascending=[True, True])\n",
    "df3.to_csv('./data/intermediateData/04_PdM_telemetry_model_age_07_maint_err_fail_final_RUL_asc_pre.csv')\n",
    "\n",
    "with open(\"data/intermediateData/04_PdM_telemetry_model_age_07_maint_err_fail_final_RUL_asc_pre.csv\", \"r\", newline='') as source3:\n",
    "    reader3 = csv.reader(source3)\n",
    "      \n",
    "    with open(\"data/intermediateData/05_PdM_telemetry_model_age_07_maint_err_fail_final_asc.csv\", \"w\", newline='') as result3:\n",
    "        writer3 = csv.writer(result3)\n",
    "        for r3 in reader3:\n",
    "            writer3.writerow((r3[1], r3[2], r3[3], r3[4], r3[5], r3[6], r3[7], r3[8], r3[9], r3[10], r3[11]))\n",
    "\n",
    "source3.close()\n",
    "result3.close()\n",
    "\n",
    "print('Writing of .../data/intermediateData/05_PdM_telemetry_model_age_07_maint_err_fail_final_asc.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine cycle\n",
    "print('Determine cycle is in progress...')\n",
    "\n",
    "with open(\"data/intermediateData/05_PdM_telemetry_model_age_07_maint_err_fail_final_asc.csv\", \"r\", newline='') as source4:\n",
    "    reader4 = csv.reader(source4)\n",
    "      \n",
    "    with open(\"data/intermediateData/06_PdM_telemetry_model_age_07_maint_err_fail_final_cycle.csv\", \"w\", newline='') as result4:\n",
    "        writer4 = csv.writer(result4)\n",
    "        mIDprev = 1\n",
    "        mID = 1\n",
    "        i = 1\n",
    "        for r4 in reader4:\n",
    "            if r4[0] == \"datetime\":\n",
    "                writer4.writerow((r4[0], r4[1], str('cycle'), r4[2], r4[3], r4[4], r4[5], r4[6], r4[7], r4[8], r4[9], r4[10]))\n",
    "            else:\n",
    "                mID = int(r4[1])\n",
    "                if mID != mIDprev:\n",
    "                    i = 1\n",
    "                    writer4.writerow((r4[0], r4[1], str(i), r4[2][-1:], r4[3], r4[4], r4[5], r4[6], r4[7], r4[8], r4[9], r4[10]))\n",
    "                    mIDprev = mID\n",
    "                    i = i+1\n",
    "                else:\n",
    "                    if r4[10] == str(0):\n",
    "                        writer4.writerow((r4[0], r4[1], str(i), r4[2][-1:], r4[3], r4[4], r4[5], r4[6], r4[7], r4[8], r4[9], r4[10]))\n",
    "                        i = 1\n",
    "                    else:\n",
    "                        writer4.writerow((r4[0], r4[1], str(i), r4[2][-1:], r4[3], r4[4], r4[5], r4[6], r4[7], r4[8], r4[9], r4[10]))\n",
    "                        i = i+1\n",
    "\n",
    "source4.close()\n",
    "result4.close()\n",
    "\n",
    "print('Writing of .../data/intermediateData/06_PdM_telemetry_model_age_07_maint_err_fail_final_cycle.csv completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test data\n",
    "print('Split data into train and test data is in progress...')\n",
    "\n",
    "# write train data\n",
    "with open(\"data/intermediateData/06_PdM_telemetry_model_age_07_maint_err_fail_final_cycle.csv\", \"r\", newline='') as source5:\n",
    "    reader5 = csv.reader(source5)\n",
    "      \n",
    "    with open(\"data/preparedData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_train.csv\", \"w\", newline='') as result5:\n",
    "        writer5 = csv.writer(result5)\n",
    "        for r5 in reader5:\n",
    "            if r5[1] not in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer5.writerow(r5)\n",
    "            \n",
    "source5.close()\n",
    "result5.close()\n",
    "\n",
    "print('Writing of .../data/preparedData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_train.csv completed successfully.')\n",
    "\n",
    "# write test data\n",
    "with open(\"data/intermediateData/06_PdM_telemetry_model_age_07_maint_err_fail_final_cycle.csv\", \"r\", newline='') as source6:\n",
    "    reader6 = csv.reader(source6)\n",
    "      \n",
    "    with open(\"data/intermediateData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_test.csv\", \"w\", newline='') as result6:\n",
    "        writer6 = csv.writer(result6)\n",
    "        for r6 in reader6:\n",
    "            if r6[0] == \"datetime\" or r6[1] in (str(92),str(99),str(97),str(98),str(89),str(93),str(91),str(100)):\n",
    "                writer6.writerow(r6)\n",
    "            \n",
    "source6.close()\n",
    "result6.close()\n",
    "\n",
    "print('Writing of .../data/intermediateData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_test.csv completed successfully.')\n",
    "\n",
    "print('Split data into train and test data completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data \n",
    "train_df = pd.read_csv('./data/intermediateData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_train.csv', sep=\",\", header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_df = pd.read_csv('./data/intermediateData/PdM_telemetry_model_age_07_maint_err_fail_final_RUL_test.csv', sep=\",\", header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RUL, we can create a label indicating time to failure. We define a boolean (`True\\False`) value for `label1` indicating the engine will fail within 30 days (RUL $<= 30$). We can also define a multiclass `label2` $\\in \\{0, 1, 2\\}$ indicating {Healthy, RUL <=30, RUL <=15} cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate label columns for training data\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "\n",
    "# Label1 indicates a failure will occur within the next 30 cycles.\n",
    "# 1 indicates failure, 0 indicates healthy \n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "\n",
    "# label2 is multiclass, value 1 is identical to label1,\n",
    "# value 2 indicates failure within 15 cycles\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) , cycle column is also used for training so we will also include the cycle column. Here, we normalize the columns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - train - step 1\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "train_df.drop(train_df.columns[[0]], axis=1, inplace=True)\n",
    "train_df.drop(train_df.columns[[8]], axis=1, inplace=True)\n",
    "train_df.drop(train_df.columns[[8]], axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization - train - step 2\n",
    "cols_normalize = train_df.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the test data. We normalize the data using the same parameters from the training data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - test - step 1\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "test_df.drop(test_df.columns[[0]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[8]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[8]], axis=1, inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalizatio - test - step 2\n",
    "cols_normalize = test_df.columns.difference(['machineID','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=test_df.index)\n",
    "join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = join_df.reindex(columns = test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the same labels as used for the `training` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "One critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering as computing rolling averages over large window sizes (i.e. 50 cycles) may lead to loss of information due to smoothing and abstracting of values over such a long period. While feature engineering over large window sizes may not make sense, LSTMs are able to use all the information in the window as input.\n",
    "\n",
    "We first look at an example of the sensor values for 50 cycles prior to the failure for engine `id = 3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for visualizations \n",
    "# window of 50 cycles prior to a failure point for machibe id 89\n",
    "engine_id89 = test_df[test_df['machineID'] == 89]\n",
    "engine_id89_50cycleWindow = engine_id89[engine_id89['RUL'] <= engine_id89['RUL'].min() + 50]\n",
    "cols1 = ['volt', 'rotate']\n",
    "engine_id89_50cycleWindow1 = engine_id89_50cycleWindow[cols1]\n",
    "cols2 = ['pressure', 'vibration']\n",
    "engine_id89_50cycleWindow2 = engine_id89_50cycleWindow[cols2]\n",
    "\n",
    "# plotting sensor data for machine ID 89 prior to a failure point \n",
    "ax1 = engine_id89_50cycleWindow1.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting sensor data for machine ID 89 prior to a failure point \n",
    "ax2 = engine_id89_50cycleWindow2.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data sets\n",
    "\n",
    "The `Code\\2_model_building_and_evaluation.ipynb` Jupyter notebook will read these data files and train a LSTM network to predict the probability of machine failure within the next 30 cycles using the previous n cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data saving into files in progress...')\n",
    "\n",
    "train_df.to_csv('./data/preparedData/PM_train_prepared.csv', encoding='utf-8', index=False)\n",
    "test_df.to_csv('./data/preparedData/PM_test_prepared.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(\"Data files saved!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "f1edc3b7e2bc1b056dd3afabe23c0a81e5f58df78ee98050cdc1bb46216e89a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
